import os
import glob
import json
from datetime import datetime, timedelta

RESULTS_DIR = "results"
RESULTS_DIR_AGENT = "results_agent"
DATABASE_DIR = "database"
DATABASE_FILE = "papers_db.json"

def normalize_key(paper):
    """
    Chuáº©n hÃ³a key Ä‘á»ƒ so sÃ¡nh trÃ¹ng láº·p:
    - Æ¯u tiÃªn DOI (lowercase, bá» khoáº£ng tráº¯ng).
    - Náº¿u khÃ´ng cÃ³ DOI â†’ dÃ¹ng link.
    - Náº¿u khÃ´ng cÃ³ link â†’ dÃ¹ng title.
    """
    doi = (paper.get("doi") or "").strip().lower()
    link = (paper.get("link") or "").strip().lower()
    title = (paper.get("title") or "").strip().lower()

    if doi:
        return doi
    elif link:
        return link
    elif title:
        return title
    return ""

# ==============================
# Láº¥y file JSON má»›i nháº¥t
# ==============================
def get_latest_json():
    """
    Láº¥y file JSON má»›i nháº¥t theo ngÃ y cÃ³ dáº¡ng: YYYY-MM-DD_allapi_scholar_ndt.json
    """
    pattern = os.path.join(RESULTS_DIR, "*_allapi_scholar_ndt.json")
    json_files = glob.glob(pattern)

    if not json_files:
        print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y file JSON nÃ o trong thÆ° má»¥c results/")
        return None

    # Láº¥y ngÃ y tá»« tÃªn file vÃ  chá»n ngÃ y má»›i nháº¥t
    files_with_dates = []
    for f in json_files:
        base = os.path.basename(f)
        try:
            date_part = base.split("_")[0]  # Láº¥y pháº§n YYYY-MM-DD
            datetime.strptime(date_part, "%Y-%m-%d")  # kiá»ƒm tra format
            files_with_dates.append((date_part, f))
        except Exception:
            continue

    if not files_with_dates:
        print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y file JSON há»£p lá»‡ theo ngÃ y")
        return None

    # Chá»n file cÃ³ ngÃ y má»›i nháº¥t
    latest_file = max(files_with_dates, key=lambda x: x[0])[1]
    print(f"ğŸ“‚ File JSON má»›i nháº¥t theo ngÃ y: {latest_file}")
    return latest_file

def get_latest_json_agent():
    """
    Láº¥y file JSON má»›i nháº¥t theo ngÃ y cÃ³ dáº¡ng: YYYY-MM-DD_allapi_scholar_ndt.json
    """
    pattern = os.path.join(RESULTS_DIR_AGENT, "*_allapi_scholar_ndt.json")
    json_files = glob.glob(pattern)

    if not json_files:
        print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y file JSON nÃ o trong thÆ° má»¥c results_agent/")
        return None

    # Láº¥y ngÃ y tá»« tÃªn file vÃ  chá»n ngÃ y má»›i nháº¥t
    files_with_dates = []
    for f in json_files:
        base = os.path.basename(f)
        try:
            date_part = base.split("_")[0]  # Láº¥y pháº§n YYYY-MM-DD
            datetime.strptime(date_part, "%Y-%m-%d")  # kiá»ƒm tra format
            files_with_dates.append((date_part, f))
        except Exception:
            continue

    if not files_with_dates:
        print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y file JSON há»£p lá»‡ theo ngÃ y")
        return None

    # Chá»n file cÃ³ ngÃ y má»›i nháº¥t
    latest_file = max(files_with_dates, key=lambda x: x[0])[1]
    print(f"ğŸ“‚ File JSON má»›i nháº¥t theo ngÃ y: {latest_file}")
    return latest_file

# ==============================
# LÆ°u file JSON vá»›i timestamp
# ==============================
def save_results_to_json(data, output_dir=RESULTS_DIR, prefix="allapi_scholar_ndt"):
    """
    LÆ°u káº¿t quáº£ vÃ o file JSON vá»›i tÃªn chá»©a timestamp.
    Náº¿u cÃ¹ng 1 ngÃ y Ä‘Ã£ cÃ³ file -> load dá»¯ liá»‡u cÅ©, merge thÃªm dá»¯ liá»‡u má»›i (lá»c trÃ¹ng), rá»“i ghi Ä‘Ã¨ láº¡i.
    """
    os.makedirs(output_dir, exist_ok=True)
    today_str = datetime.now().strftime("%Y-%m-%d")

    # TÃ¬m file trong ngÃ y hÃ´m nay
    existing_file = None
    for fname in os.listdir(output_dir):
        if fname.startswith(today_str) and fname.endswith(f"{prefix}.json"):
            existing_file = os.path.join(output_dir, fname)
            break

    merged_data = []
    if existing_file:
        try:
            with open(existing_file, "r", encoding="utf-8") as f:
                old_data = json.load(f)
        except Exception as e:
            print(f"âš ï¸ Lá»—i khi Ä‘á»c file cÅ© {existing_file}: {e}")
            old_data = []
        merged_data = old_data
    else:
        # Náº¿u chÆ°a cÃ³ file -> táº¡o file má»›i
        timestamp = datetime.now().strftime("%Y-%m-%d")
        filename = f"{timestamp}_{prefix}.json"
        existing_file = os.path.join(output_dir, filename)

    # Merge dá»¯ liá»‡u (lá»c trÃ¹ng theo key chuáº©n hÃ³a)
    existing_keys = {normalize_key(item) for item in merged_data if normalize_key(item)}
    new_filtered = [p for p in data if normalize_key(p) not in existing_keys]

    if not new_filtered:
        print("â© KhÃ´ng cÃ³ dá»¯ liá»‡u má»›i Ä‘á»ƒ thÃªm.")
        return existing_file

    merged_data.extend(new_filtered)

    try:
        with open(existing_file, "w", encoding="utf-8") as f:
            json.dump(merged_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ÄÃ£ cáº­p nháº­t file: {existing_file} (thÃªm {len(new_filtered)} bÃ i bÃ¡o)")
        return existing_file
    except Exception as e:
        print(f"âŒ Lá»—i khi lÆ°u file JSON: {e}")
        return None


# ==============================
# Load Database DOI
# ==============================
def load_database(db_dir=DATABASE_DIR, db_file=DATABASE_FILE):
    os.makedirs(db_dir, exist_ok=True)
    db_path = os.path.join(db_dir, db_file)

    if os.path.exists(db_path):
        with open(db_path, "r", encoding="utf-8") as f:
            return json.load(f)
    else:
        return []


def save_database(data, db_dir=DATABASE_DIR, db_file=DATABASE_FILE):
    os.makedirs(db_dir, exist_ok=True)
    db_path = os.path.join(db_dir, db_file)

    with open(db_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ Database Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t: {db_path}")

# ==============================
# Cáº­p nháº­t Database (lÆ°u Title + DOI)
# ==============================
def save_results_to_database(result_file, db_dir=DATABASE_DIR, db_file=DATABASE_FILE):
    """
    Äá»c káº¿t quáº£ tá»« file JSON vÃ  lÆ°u vÃ o database.
    Chuáº©n hÃ³a key (doi/link/title) vÃ  loáº¡i bá» trÃ¹ng láº·p.
    """
    if not os.path.exists(result_file):
        print(f"âŒ File káº¿t quáº£ khÃ´ng tá»“n táº¡i: {result_file}")
        return False

    try:
        with open(result_file, "r", encoding="utf-8") as f:
            results = json.load(f)
    except Exception as e:
        print(f"âŒ Lá»—i khi Ä‘á»c file káº¿t quáº£ {result_file}: {e}")
        return False

    db_data = load_database(db_dir, db_file)
    db_dict = {normalize_key(item): item for item in db_data if normalize_key(item)}

    new_count = 0
    for paper in results:
        key = normalize_key(paper)
        if key and key not in db_dict:
            db_dict[key] = {
                "title": paper.get("title", "Untitled"),
                "doi": paper.get("doi", paper.get("link", ""))
            }
            new_count += 1

    save_database(list(db_dict.values()), db_dir, db_file)
    print(f"âœ… ÄÃ£ thÃªm {new_count} bÃ i bÃ¡o má»›i vÃ o database tá»« {result_file}")
    return True



# ==============================
# Lá»c bÃ i bÃ¡o trÃ¹ng 
# ==============================
def filter_duplicates(new_results, results_dir=RESULTS_DIR, db_dir=DATABASE_DIR, db_file=DATABASE_FILE):
    """
    Lá»c trÃ¹ng cÃ¡c bÃ i bÃ¡o má»›i báº±ng key chuáº©n hÃ³a (doi/link/title).
    - Náº¿u file má»›i nháº¥t lÃ  hÃ´m nay â†’ khÃ´ng lá»c.
    - Náº¿u file má»›i nháº¥t lÃ  hÃ´m qua â†’ lá»c theo hÃ´m qua.
    - Náº¿u khÃ´ng pháº£i hÃ´m nay vÃ  khÃ´ng pháº£i hÃ´m qua â†’ lá»c theo database.
    """
    today_str = datetime.now().strftime("%Y-%m-%d")
    yesterday_str = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")

    # ğŸ”¹ Láº¥y file JSON má»›i nháº¥t
    latest_file = get_latest_json()
    if not latest_file:
        return new_results

    # ğŸ”¹ Äá»c dá»¯ liá»‡u file má»›i nháº¥t
    try:
        with open(latest_file, "r", encoding="utf-8") as f:
            old_results = json.load(f)
    except Exception as e:
        print(f"âŒ Lá»—i khi Ä‘á»c file {latest_file}: {e}")
        return new_results

    old_dates = {paper.get("pub_date", "") for paper in old_results}

    # âœ… File hÃ´m nay â†’ khÃ´ng lá»c
    if today_str in old_dates:
        print("â© File má»›i nháº¥t Ä‘Ã£ lÃ  hÃ´m nay -> KhÃ´ng lá»c trÃ¹ng.")
        return new_results

    # âœ… KhÃ´ng pháº£i hÃ´m nay â†’ lá»c
    # Náº¿u lÃ  hÃ´m qua â†’ lá»c theo hÃ´m qua
    if yesterday_str in old_dates:
        old_keys = {normalize_key(p) for p in old_results if normalize_key(p)}
        filtered_results = [p for p in new_results if normalize_key(p) not in old_keys]
        removed_count = len(new_results) - len(filtered_results)
        print(f"ğŸ—‘ï¸ ÄÃ£ loáº¡i bá» {removed_count} bÃ i bÃ¡o trÃ¹ng vá»›i hÃ´m qua.")
        return filtered_results

    # âœ… KhÃ´ng pháº£i hÃ´m qua â†’ lá»c theo database
    db_path = os.path.join(db_dir, db_file)
    if not os.path.exists(db_path):
        print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y database -> Tráº£ vá» toÃ n bá»™ dá»¯ liá»‡u má»›i.")
        return new_results

    try:
        with open(db_path, "r", encoding="utf-8") as f:
            db_data = json.load(f)
            db_keys = {normalize_key(item) for item in db_data if normalize_key(item)}
    except Exception as e:
        print(f"âŒ Lá»—i khi Ä‘á»c database {db_path}: {e}")
        return new_results

    filtered_results = [p for p in new_results if normalize_key(p) not in db_keys]
    removed_count = len(new_results) - len(filtered_results)
    print(f"ğŸ—‘ï¸ ÄÃ£ loáº¡i bá» {removed_count} bÃ i bÃ¡o trÃ¹ng vá»›i database.")
    return filtered_results
