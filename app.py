import streamlit as st
from springer_search import run_springer_search
from mdpi_search import run_mdpi_search
from scholar_search import run_scholar_search
from search_api import search_openalex, search_semantic_scholar, search_arxiv, search_crossref,enrich_with_firecrawl, summarize_filtered_papers, filter_irrelevant_papers
import pandas as pd
from datetime import datetime

import json
import os
import glob
import asyncio
import sys
from dotenv import load_dotenv, set_key

# ===================== WINDOWS FIX =====================
if sys.platform.startswith("win"):
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

# ===================== PAGE CONFIG =====================
st.set_page_config(page_title="Paper Search App", layout="wide")

# -------------------- CONFIG FILE --------------------
RESULTS_DIR = "results"
ENV_PATH = ".env"

if not os.path.exists(RESULTS_DIR):
    os.makedirs(RESULTS_DIR)

if not os.path.exists(ENV_PATH):
    open(ENV_PATH, "a").close()

load_dotenv(ENV_PATH)

# ===================== SUPPORT FUNCTION =====================
def merge_and_save_results(all_results, filename):
    """
    H·ª£p nh·∫•t k·∫øt qu·∫£ t·ª´ nhi·ªÅu ngu·ªìn, lo·∫°i b·ªè b√†i b√°o tr√πng nhau d·ª±a v√†o ti√™u ƒë·ªÅ.
    """
    seen_titles = set()
    merged_results = []

    for source_data in all_results:
        for paper in source_data:
            title = paper.get("title", "").strip().lower()
            if title and title not in seen_titles:
                merged_results.append(paper)
                seen_titles.add(title)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")  # v√≠ d·ª•: 20250919_103045
    filename_with_time = f"{timestamp}_{filename}"

    filepath = os.path.join(RESULTS_DIR, filename_with_time)
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(merged_results, f, indent=2, ensure_ascii=False)

    return filepath, merged_results

# ===================== TABS =====================
tab1, tab2, tab3 = st.tabs([
    "üåê All APIs + Scholar",
    "üìò Springer + MDPI",
    "üìÅ Danh s√°ch k·∫øt qu·∫£"
])

# ===================== TAB 1 =====================
with tab1:
    st.subheader("üîπ T√¨m ki·∫øm tr√™n All APIs + Google Scholar")

    # Nh·∫≠p t·ª´ kh√≥a v√† s·ªë l∆∞·ª£ng b√†i m·ªôt l·∫ßn
    keyword_tab1 = st.text_input("Nh·∫≠p t·ª´ kh√≥a t√¨m ki·∫øm (All APIs + Scholar):", key="keyword_tab1")
    max_results_tab1 = st.number_input("S·ªë l∆∞·ª£ng b√†i mu·ªën l·∫•y m·ªói ngu·ªìn", min_value=1, max_value=20, value=10, key="max_results_tab1")

    if st.button("üîç T√¨m ki·∫øm All APIs + Scholar"):
        if not keyword_tab1.strip():
            st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p t·ª´ kh√≥a t√¨m ki·∫øm!")
        else:
            with st.spinner("ƒêang t√¨m ki·∫øm tr√™n t·∫•t c·∫£ c√°c API..."):
                # G·ªçi API
                openalex_res = search_openalex(query=keyword_tab1, rows=max_results_tab1)
                # semantic_res = search_semantic_scholar(query=keyword_tab2, rows=max_results_tab2)
                arxiv_res = search_arxiv(query=keyword_tab1, rows=max_results_tab1)
                crossref_res = search_crossref(query=keyword_tab1, rows=max_results_tab1)

                # Google Scholar
                scholar_data = run_scholar_search(keyword_tab1, max_results_tab1)

                # Merge k·∫øt qu·∫£
                merged_results = []
                for res in [openalex_res, arxiv_res, crossref_res, scholar_data]:
                    merged_results.extend(res)

                # Crawl b·ªï sung b·∫±ng Firecrawl
                st.info("‚è≥ ƒêang crawl abstract b·ªï sung b·∫±ng Firecrawl...")
                enriched_results = enrich_with_firecrawl(merged_results)
                filter_results = filter_irrelevant_papers(enriched_results)
                summarize_results = summarize_filtered_papers(filter_results)

                # L∆∞u enriched
                current_time = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
                filename = f"{current_time}_allapi_scholar_{keyword_tab1.replace(' ', '_')}.json"
                merged_file = os.path.join(RESULTS_DIR, filename)
                with open(merged_file, "w", encoding="utf-8") as f:
                    json.dump(summarize_results, f, indent=2, ensure_ascii=False)

                # Hi·ªÉn th·ªã enriched tr·ª±c ti·∫øp
                st.success(f"‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ enriched v√†o: {merged_file}")
                df = pd.DataFrame(summarize_results)
                st.dataframe(df)

                st.download_button(
                    label="üì• T·∫£i k·∫øt qu·∫£ JSON",
                    data=json.dumps(summarize_results, indent=2, ensure_ascii=False),
                    file_name=os.path.basename(merged_file),
                    mime="application/json"
                )




# ===================== TAB 2 =====================
with tab2:
    st.subheader("üîπ T√¨m ki·∫øm tr√™n Springer v√† MDPI")

    # --- Nh·∫≠p Google API Key ch·ªâ hi·ªÉn th·ªã t·∫°i ƒë√¢y ---
    st.markdown("### ‚öôÔ∏è C·∫•u h√¨nh Google API Key")
    gg_api_key = st.text_input(
        "Nh·∫≠p Google API Key",
        type="password",
        placeholder="Nh·∫≠p API Key..."
    )

    if st.button("üíæ L∆∞u Google API Key"):
        if gg_api_key.strip():
            set_key(ENV_PATH, "GOOGLE_API_KEY", gg_api_key.strip())
            st.success("‚úÖ API Key ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o file `.env`")
        else:
            st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p API Key tr∆∞·ªõc khi l∆∞u")

    # Nh·∫≠p t·ª´ kh√≥a v√† s·ªë l∆∞·ª£ng b√†i m·ªôt l·∫ßn
    keyword_tab2 = st.text_input("Nh·∫≠p t·ª´ kh√≥a t√¨m ki·∫øm (Springer + MDPI):", key="keyword_tab2")
    max_results_tab2 = st.number_input("S·ªë l∆∞·ª£ng b√†i mu·ªën l·∫•y m·ªói ngu·ªìn", min_value=1, max_value=20, value=10, key="max_results_tab2")

    if st.button("üîç T√¨m ki·∫øm Springer + MDPI"):
        if not keyword_tab2.strip():
            st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p t·ª´ kh√≥a t√¨m ki·∫øm!")
        else:
            with st.spinner("ƒêang t√¨m ki·∫øm tr√™n Springer v√† MDPI..."):
                springer_result = run_springer_search(keyword_tab2, max_results_tab2)
                mdpi_result = run_mdpi_search(keyword_tab2, max_results_tab2)

                springer_data = [] if "error" in springer_result else springer_result["data"]
                mdpi_data = [] if "error" in mdpi_result else mdpi_result["data"]

                if "error" in springer_result:
                    st.error(f"Springer: {springer_result['error']}")
                if "error" in mdpi_result:
                    st.error(f"MDPI: {mdpi_result['error']}")

                # H·ª£p nh·∫•t k·∫øt qu·∫£
                merged_results = springer_data + mdpi_data

                # enrich b·∫±ng Firecrawl
                st.info("‚è≥ ƒêang crawl abstract b·ªï sung b·∫±ng Firecrawl...")
                enriched_results = enrich_with_firecrawl(merged_results)
                filter_results = filter_irrelevant_papers(enriched_results)
                summarize_results = summarize_filtered_papers(filter_results)

                # L∆∞u k·∫øt qu·∫£ ch·ªâ sau khi ƒë√£ l√†m gi√†u xong
                current_time = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
                filename = f"{current_time}_springer_mdpi_{keyword_tab2.replace(' ', '_')}.json"
                merged_file = os.path.join(RESULTS_DIR, filename)

                # L∆∞u k·∫øt qu·∫£ v√†o file
                with open(merged_file, "w", encoding="utf-8") as f:
                    json.dump(summarize_results, f, indent=2, ensure_ascii=False)
                st.success(f"‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ h·ª£p nh·∫•t v√†o: {merged_file}")

                df = pd.DataFrame(enriched_results)
                st.dataframe(df)

                st.download_button(
                    label="üì• T·∫£i k·∫øt qu·∫£ JSON",
                    data=open(merged_file, "rb").read(),
                    file_name=os.path.basename(merged_file),
                    mime="application/json"
                )


# ===================== TAB 3 =====================
with tab3:
    st.subheader("üìÇ Danh s√°ch t·∫•t c·∫£ file k·∫øt qu·∫£ ƒë√£ l∆∞u")

    files = sorted(glob.glob(os.path.join(RESULTS_DIR, "*.json")), key=os.path.getmtime, reverse=True)

    if not files:
        st.info("‚ö†Ô∏è Ch∆∞a c√≥ file k·∫øt qu·∫£ n√†o ƒë∆∞·ª£c l∆∞u.")
    else:
        for file_path in files:
            filename = os.path.basename(file_path)
            file_size = os.path.getsize(file_path) / 1024  # KB
            st.write(f"**üìÑ {filename}** ({file_size:.2f} KB)")

            with open(file_path, "rb") as f:
                st.download_button(
                    label=f"üì• T·∫£i {filename}",
                    data=f.read(),
                    file_name=filename,
                    mime="application/json",
                    key=filename
                )
            st.markdown("---")
